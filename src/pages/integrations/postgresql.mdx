---
title: PostgreSQL
metaTitle: Learn How to Integrate and Configure Filebeat to Send PostgreSQL Logs
subTitle: Collect and ship PostgreSQL logs to Logstash and Elasticsearch
logo: postgresqlelephant
color: "#336791"
description: How to send PostgreSQL logs to your Hosted ELK Logstash instance. Configure Filebeat to send PostgreSQL logs to Logstash or Elasticsearch. 
stackTypes: logs, metrics
sslPortType: beats-ssl
tags: Logs, Database, DB, PostgreSQL, RDBMS, SQL, Postgresql, Telegraf, Metrics, Telemetry, OpenTelemetry, Health, Instrumentation, Monitoring, Database Management
dashboardIds: postgresql
---

Follow the steps below to send your observability data to Logit.io

<Steps>
  ## Logs

  Filebeat is a lightweight shipper that enables you to send your PostgreSQL 
  application logs to Logstash and Elasticsearch. Configure Filebeat using the 
  pre-defined examples below to start sending and analysing your PostgreSQL application logs.

  ### Install Integration
  <InstallIntegration/>
  ### Install Filebeat

  <InstallFilebeat />

  ### Enable the PostgreSQL module

  <Tabs items={["Windows", "Linux", "macOS", "DEB", "RPM"]}>
    <Tab>
      There are several built in filebeat modules you can use. You will need to enable the postgresql module:

      ```cmd copy
      .\filebeat.exe modules list
      .\filebeat.exe modules enable postgresql
      ```
      In the module config under modules.d, change the module settings to match your environment. You must enable at least one fileset in the module. 
      
      ___Filesets are disabled by default___.

      Copy the snippet below and replace the contents of the postgresql.yml module file:

      ```yml copy
      # Module: postgresql
      # Docs: https://www.elastic.co/guide/en/beats/filebeat/8.12/filebeat-module-postgresql.html

      - module: postgresql
        # All logs
        log:
          enabled: true

          # Set custom paths for the log files. If left empty,
          # Filebeat will choose the paths depending on your OS.
          #var.paths:
      ```
    </Tab>
    <Tab>
      There are several built in filebeat modules you can use. You will need to enable the postgresql module:

      ```cmd copy
      sudo filebeat modules list
      sudo filebeat modules enable postgresql
      ```
      In the module config under modules.d, change the module settings to match your environment. You must enable at least one fileset in the module. 
      
      ___Filesets are disabled by default___.

      Copy the snippet below and replace the contents of the postgresql.yml module file:

      ```yml copy
      # Module: postgresql
      # Docs: https://www.elastic.co/guide/en/beats/filebeat/8.12/filebeat-module-postgresql.html

      - module: postgresql
        # All logs
        log:
          enabled: true

          # Set custom paths for the log files. If left empty,
          # Filebeat will choose the paths depending on your OS.
          #var.paths:
      ```
    </Tab>
    <Tab>
      There are several built in filebeat modules you can use. You will need to enable the postgresql module:

      ```cmd copy
      ./filebeat modules list
      ./filebeat modules enable postgresql
      ```
      In the module config under modules.d, change the module settings to match your environment. You must enable at least one fileset in the module. 
      
      ___Filesets are disabled by default___.

      Copy the snippet below and replace the contents of the postgresql.yml module file:

      ```yml copy
      # Module: postgresql
      # Docs: https://www.elastic.co/guide/en/beats/filebeat/8.12/filebeat-module-postgresql.html

      - module: postgresql
        # All logs
        log:
          enabled: true

          # Set custom paths for the log files. If left empty,
          # Filebeat will choose the paths depending on your OS.
          #var.paths:
      ```
    </Tab>
    <Tab>
      There are several built in filebeat modules you can use. You will need to enable the postgresql module:

      ```cmd copy
      sudo filebeat modules list
      sudo filebeat modules enable postgresql
      ```
      In the module config under modules.d, change the module settings to match your environment. You must enable at least one fileset in the module. 
      
      ___Filesets are disabled by default___.

      Copy the snippet below and replace the contents of the postgresql.yml module file:

      ```yml copy
      # Module: postgresql
      # Docs: https://www.elastic.co/guide/en/beats/filebeat/8.12/filebeat-module-postgresql.html

      - module: postgresql
        # All logs
        log:
          enabled: true

          # Set custom paths for the log files. If left empty,
          # Filebeat will choose the paths depending on your OS.
          #var.paths:
      ```
    </Tab>
    <Tab>
      There are several built in filebeat modules you can use. You will need to enable the postgresql module:

      ```cmd copy
      sudo filebeat modules list
      sudo filebeat modules enable postgresql
      ```
      In the module config under modules.d, change the module settings to match your environment. You must enable at least one fileset in the module. 
      
      ___Filesets are disabled by default___.
      
      Copy the snippet below and replace the contents of the postgresql.yml module file:

      ```yml copy
      # Module: postgresql
      # Docs: https://www.elastic.co/guide/en/beats/filebeat/8.12/filebeat-module-postgresql.html

      - module: postgresql
        # All logs
        log:
          enabled: true

          # Set custom paths for the log files. If left empty,
          # Filebeat will choose the paths depending on your OS.
          #var.paths:
      ```
    </Tab>
  </Tabs>

  ### Update Your Configuration File

  <UpdateFilebeatConfigFile />

  ### Validate configuration

  <ValidateBeat beatname="filebeat" />
  
  ### Start filebeat

  <StartFilebeat />

  ### Check Logit.io for your logs
  <LaunchStack utmMedium="logs" utmCampaign="filebeat" source="filebeat" />

  ### How to diagnose no data in Stack

  <DiagnoseNoData />

  ### PostgreSQL Dashboard

  The PostgreSQL module comes with predefined Kibana dashboards. To view your dashboards for any of your Logit.io stacks, launch Logs and choose Dashboards.
          
  ![Predefined kibana dashboard screenshot](@/images/integrations/filebeat/postgresql.png)
</Steps>

<Steps>
  ## Metrics

  Configure Telegraf to ship PostgreSQL metrics to your Logit.io stacks via Logstash.
  
  ### Install Integration
  <InstallIntegration/>
  ### Install Telegraf

  <InstallTelegraf />

  ### Configure the Telegraf input plugin

  The configuration file below is pre-configured to scrape the system metrics from your hosts, add the following code to the configuration file `/etc/telegraf/telegraf.conf` from the previous step.

  ```toml copy
  # Read metrics from one or many postgresql servers
  [[inputs.postgresql]]
    ## Specify address via a url matching:
    ##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]&statement_timeout=...
    ## or a simple string:
    ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production
    ## Users can pass the path to the socket as the host value to use a socket
    ## connection (e.g. `/var/run/postgresql`).
    ##
    ## All connection parameters are optional.
    ##
    ## Without the dbname parameter, the driver will default to a database
    ## with the same name as the user. This dbname is just for instantiating a
    ## connection with the server and doesn't restrict the databases we are trying
    ## to grab metrics for.
    ##
    address = "host=localhost user=postgres sslmode=disable"

    ## A custom name for the database that will be used as the "server" tag in the
    ## measurement output. If not specified, a default one generated from
    ## the connection address is used.
    # outputaddress = "db01"

    ## connection configuration.
    ## maxlifetime - specify the maximum lifetime of a connection.
    ## default is forever (0s)
    ##
    ## Note that this does not interrupt queries, the lifetime will not be enforced
    ## whilst a query is running
    # max_lifetime = "0s"

    ## A  list of databases to explicitly ignore.  If not specified, metrics for all
    ## databases are gathered.  Do NOT use with the 'databases' option.
    # ignored_databases = ["postgres", "template0", "template1"]

    ## A list of databases to pull metrics about. If not specified, metrics for all
    ## databases are gathered.  Do NOT use with the 'ignored_databases' option.
    # databases = ["app_production", "testing"]

    ## Whether to use prepared statements when connecting to the database.
    ## This should be set to false when connecting through a PgBouncer instance
    ## with pool_mode set to transaction.
    prepared_statements = true
  ```
  <Callout type="info">
    Read more about how to configure data scraping and configuration options for [PostgreSQL](https://github.com/influxdata/telegraf/tree/master/plugins/inputs/postgresql)
  </Callout>

  ### Configure the output plugin

  <TelegrafOutputPlugin />

  ### Start Telegraf

  <StartTelegraf />

  ### View your metrics
  <LaunchStack source="PostgreSQL_Metrics_via_Telegraf" utmMedium="metrics" utmCampaign="telegraf-PostgreSQL-metrics" />

  ### How to diagnose no data in Stack

  <DiagnoseNoData />

</Steps>

### PostgreSQL Logging Overview

PostgreSQL (often shortened to Postgres) is a highly stable open-source relational database 
that supports both relational & non-relational querying. Postgres can run across the majority 
of operating systems including Linux, [Windows](/integrations/windows) 
and [macOS](/integrations/macos).

PostgreSQL is used by some of the world's best known brands including Apple, IMDB, Red Hat & Cisco 
due to its robust feature set, useful addons & scalability. 

Some of the benefits of using this database include their support for the majority of programming 
languages as well as it's strengths as a reliable transactional database for companies of all sizes.

PostgreSQL users are encouraged to log as much as possible as with insufficient configuration you could 
easily lose access to key messages for troubleshooting and error resolution. Below are some of the most 
important logs you'll likely need to analyse when running Postgres.

PostgreSQL transaction logs help the user to identify what queries a transaction encountered.

Remote Host IP/Name (w/ port) logs can serve to help security technicians identify suspicious activity 
that has occurred. If you are looking to pinpoint troublesome sessions affecting your infrastructure 
you might turn to Process ID logs for further insights.

When it comes to logging in Postgres there are twenty three other parameters which can be isolated for 
troubleshooting using the various keywords; ERROR, FATAL, WARNING, & PANIC.

With all these logs, directories & parameters it is easy to become overwhelmed at the prospect of having 
to thoroughly analyse your log data & you may wish to use a 
[log management system](https://logit.io/platform/logging/log-management) to streamline your processes. 

Our built in PostgreSQL log file analyser helps DBAs, sysadmins, and developers identify issues, create 
visualisations & set alerts when preconfigured and custom parameters are met.

If you need any assistance with analysing your PostgreSQL logs we're here to help. 
Feel free to reach out by contacting the Logit.io support 
team via <IntercomButton text="live chat"/> & we'll be happy to help you start analysing your data.